{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a59b58",
   "metadata": {},
   "source": [
    "<b>Data mining Project - 2021/22</b><br/>\n",
    "<span>\n",
    "<b>Authors:</b> Mariagiovanna Rotundo (560765), Nunzio Lopardo (600005)</a> and Renato Eschini (203021)<br/>\n",
    "<b>Group:</b>3<br/>\n",
    "<b>Release date:</b> 26/12/2021\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04b76a",
   "metadata": {},
   "source": [
    "# Task 2: Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bede9",
   "metadata": {},
   "source": [
    "This workbook contains a clustering analysis on the tennis dataset.\n",
    "This dataset was derived from the \"tennis_matches.csv\" dataset by deriving player information from the matches played.\n",
    "\n",
    "A new \"performance\" dataset was also derived in which the data was reprocessed to use only essential and important information for a player-related clustering analysis, eg. only players who have played more than a certain number of games will be considered, only numerical attributes, percentages strictly greater than zero, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd3531",
   "metadata": {},
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import date\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score, pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pyclustering.cluster.xmeans import xmeans\n",
    "from pyclustering.cluster.gmeans import gmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.cluster import cluster_visualizer, cluster_visualizer_multidim\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42114975",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05a722",
   "metadata": {},
   "source": [
    "Load \"players.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ef4a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load of the data\n",
    "DATASET_DIR = \"dataset\" + os.path.sep\n",
    "#index_col=False say to not use the first column as ID\n",
    "df_players = pd.read_csv('players.csv', sep=',', index_col=0) \n",
    "df_players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0764821",
   "metadata": {},
   "source": [
    "Print info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33fee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e67c0",
   "metadata": {},
   "source": [
    "## Perfomances dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08db675",
   "metadata": {},
   "source": [
    "We analyze the distribution of players by number of matches played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d964765",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=df_players['nmatch'], bins=\"auto\", binrange=(10,400), color=\"lightgreen\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d080046",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_players['best_rank'], bins=\"auto\", color=\"lightgreen\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3708c",
   "metadata": {},
   "source": [
    "We create the performance dataset by taking only some attributes and the players who have at least a fixed number of n_match games (for example at least 100 games played)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea0a17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_match = 100\n",
    "\n",
    "df_performances_org = df_players[\n",
    "    (df_players['best_rank']>0) & \n",
    "    (df_players['best_rank_points']>=0) & \n",
    "    (df_players['tot_minutes']>0) & \n",
    "    (df_players['ace']>=0) & \n",
    "    (df_players['bpS']>=0)][[\n",
    "'best_rank', \n",
    "'best_rank_points',                            \n",
    "'tot_minutes',\n",
    "'sv1st_win', \n",
    "'sv2nd_win', \n",
    "'df', \n",
    "'ace', \n",
    "'bpS', \n",
    "'nmatch', \n",
    "'best_of_3_match', \n",
    "'best_of_3_wins', \n",
    "'best_of_5_match', \n",
    "'best_of_5_wins', \n",
    "'wmatch',\n",
    "'lmatch', \n",
    "'w_tourney',\n",
    "'n_tourney']]\n",
    "df_performances = df_performances_org.loc[df_performances_org['nmatch'] > n_match]\n",
    "\n",
    "df_performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02b054",
   "metadata": {},
   "source": [
    "## Elimination of highly correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f999b",
   "metadata": {},
   "source": [
    "We begin by examining the correlations between the attributes of the dataset to be clustered in order to identify the highly correlated couples. Dropping redundant attributes benefits the analysis by reducing the dimensionality of the dataset and rising the influence that more useful feature could have on the whole clustering process.\n",
    "\n",
    "With such aim in mind, we fix a maximum threshold value in order to identify highly correlated features and subsequently drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bd4e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "sns.heatmap( df_performances.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f7b9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_threshold = 0.9\n",
    "print(\"Att. A\\t\\t\\tAtt. B\\t\\t\\tCorr(A,B)\")\n",
    "for i in range(0, len(df_performances.columns)):\n",
    "    for j in range(i+1, len(df_performances.columns)):\n",
    "        corr = df_performances[df_performances.columns[i]].corr(df_performances[df_performances.columns[j]])\n",
    "        if  corr > corr_threshold:\n",
    "            print(df_performances.columns[i] + \"\\t\\t\\t\" + df_performances.columns[j] + \"\\t\\t\\t\" + '{:.4f}'.format(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc22d70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#   \n",
    "corr_columns = ['best_of_3_match', 'best_of_5_match', 'sv2nd_win', 'tot_minutes']\n",
    "tmp = df_performances.drop(corr_columns, axis=1, inplace=False)\n",
    "df_performances = tmp\n",
    "df_performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc785048",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb29af7",
   "metadata": {},
   "source": [
    "## Identification of the best value of k \n",
    "The **K** parameter represents the number of clusters into which we want to divide the dataset. <br />\n",
    "There are different methods to calculate the value of K. <br />\n",
    "Below we use the **Elbow** method with a ready-made library and by simulating clustering with different k and then measuring the results obtained by **Sum of Square Errors (SSE)** and by **Silhouette**, then measuring the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3340e",
   "metadata": {},
   "source": [
    "#### KElbowVisualizer with 3 different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer with \n",
    "# mean sum of squared distances to centers (distortion) metric\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,16), metric=\"distortion\")\n",
    "\n",
    "visualizer.fit(df_performances)        # Fit the data to the visualizer\n",
    "visualizer.show()                      # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61235850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer with \n",
    "# mean ratio of intra-cluster and nearest-cluster distance (silhouette)\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2,16), metric=\"silhouette\")\n",
    "\n",
    "visualizer.fit(df_performances)        # Fit the data to the visualizer\n",
    "visualizer.show()                      # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5361d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clustering model and visualizer with \n",
    "# ratio of within to between cluster dispersion (calinski_harabasz)\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2,16), metric=\"calinski_harabasz\")\n",
    "\n",
    "visualizer.fit(df_performances)        # Fit the data to the visualizer\n",
    "visualizer.show()                      # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d41389",
   "metadata": {},
   "source": [
    "#### Simulate different clustering with different K and evaluate by hand silhouette_score and sum of standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4840190",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_list = list()\n",
    "sil_list = list()\n",
    "k_cols = []\n",
    "\n",
    "max_k = 16\n",
    "for k in tqdm(range(2, max_k + 1), total=max_k - 1, desc=\"Iterating over possible K values\"):\n",
    "    kmeans_iter = KMeans(n_clusters=k, n_init=10, max_iter=100)\n",
    "    kmeans_iter.fit(df_performances)        \n",
    "    sil_list.append(silhouette_score(df_performances, kmeans_iter.labels_))\n",
    "    sse = int(kmeans_iter.inertia_)\n",
    "    sse_list.append(sse)\n",
    "    k_cols.append(str(k))\n",
    "    \n",
    "store_results=pd.DataFrame()\n",
    "store_results['K'] = k_cols\n",
    "store_results['SSE'] = sse_list\n",
    "store_results['Silhouette'] = sil_list\n",
    "store_results.set_index(['K'], inplace=True)\n",
    "store_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cc2fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot indicators\n",
    "fig, axs = plt.subplots(2,1,figsize=(15,25))\n",
    "axs[0].plot(range(2, len(sse_list) + 2), sse_list)\n",
    "axs[0].set_ylabel('SSE', fontsize=22)\n",
    "axs[0].set_xlabel('K', fontsize=22)\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "axs[1].plot(range(2, len(sil_list) + 2), sil_list)\n",
    "axs[1].set_ylabel('Silhouette', fontsize=22)\n",
    "axs[1].set_xlabel('K', fontsize=22)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddae2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1b2aecb",
   "metadata": {},
   "source": [
    "**We can assume that this is the best value of k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801433a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3917f3ba",
   "metadata": {},
   "source": [
    "**Normalization** <br/>\n",
    "We normalize the values using MinMax in order to have the same scale on all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df_performances.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9ae7e",
   "metadata": {},
   "source": [
    "**Silhouette Viewer**\n",
    "The Silhouette Viewer displays the silhouette coefficient for each sample on a per cluster basis, visually evaluating the density and separation between clusters.\n",
    "\n",
    "In the SilhouetteVisualizer charts, clusters with higher scores have wider shapes, but less cohesive clusters will be below the average score across all clusters, which is plotted as a vertical dashed red line.\n",
    "\n",
    "This is particularly useful for determining cluster imbalance or for selecting a value for K by comparing multiple viewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters)\n",
    "visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cluster\n",
    "kmeans = KMeans(n_clusters, n_init=10, max_iter=100)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the unique labels identified by K-Means\n",
    "np.unique(kmeans.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9737f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of items for each cluster\n",
    "hist, bins = np.histogram(kmeans.labels_, bins=range(0, len(set(kmeans.labels_)) + 1))\n",
    "dict(zip(bins, hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the center of clusters\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2ef95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4114eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Clusters with Paiplot\n",
    "cleaned_dataframe = df_performances.copy()\n",
    "cleaned_dataframe[\"LABEL\"] = kmeans.labels_\n",
    "# Pairplot\n",
    "sns.pairplot(data = cleaned_dataframe, hue = \"LABEL\", palette = \"Accent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06fcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Clusters with PairGrid and in evidence the centroids of the clusters\n",
    "cleaned_dataframe = df_performances.copy()\n",
    "\n",
    "centroids_inversed = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=cleaned_dataframe.columns.values)\n",
    "cleaned_dataframe[\"label\"] = kmeans.labels_.astype(str)\n",
    "centroids_inversed[\"label\"] = [\"0 centroid\", \"1 centroid\", \"2 centroid\", \"3 centroid\"]\n",
    "full_ds = pd.concat([cleaned_dataframe, centroids_inversed], ignore_index=True)\n",
    "\n",
    "g = sns.PairGrid(\n",
    "                full_ds, \n",
    "                hue=\"label\",\n",
    "                hue_order=[\"0\", \"1\", \"2\", \"3\", \"0 centroid\", \"1 centroid\", \"2 centroid\", \"3 centroid\"],\n",
    "                palette=[\"b\", \"r\", \"y\", \"g\", \"b\", \"r\", \"y\", \"g\"],\n",
    "                hue_kws={\n",
    "                    \"s\": [20, 20, 20, 20, 500, 500, 500, 500], \n",
    "                    \"marker\": [\"o\", \"o\", \"o\", \"o\", \"*\", \"*\", \"*\", \"*\"]}\n",
    ")\n",
    "g.map(plt.scatter, linewidth=1, edgecolor=\"w\")\n",
    "g.add_legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6bc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c12104e",
   "metadata": {},
   "source": [
    "## DBscan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa9fbc",
   "metadata": {},
   "source": [
    "For the DBscan the dataset is normalized using 2 scalers (StandardScaler and MinMaxScaler) and both obtained dataset are analyzed. Then the parameters used by DBscan, eps (that is the distance where looking for the points) and and min number of point to create a cluster are analyzed to fins the best ones. So, the parameter are chosen with the analysis and a plot of clusters is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf71aa0",
   "metadata": {},
   "source": [
    "**Normalization** <br />\n",
    "We normalize the values using StandardScaler in order to have the same scale on all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48275f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_array = scaler.fit_transform(df_performances)\n",
    "scaled_dataframe = pd.DataFrame( scaled_array, columns = df_performances.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19540d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25792ebe",
   "metadata": {},
   "source": [
    "**example of clustering with fixed parameters eps and min points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe19c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.75, min_samples=5)\n",
    "dbscan.fit(scaled_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbscan.labels_\n",
    "np.unique(dbscan.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1a9bf",
   "metadata": {},
   "source": [
    "with these parameters we obtain 7 clusters (plus the cluster of not classified points) and one of them has many points while the others have few points. The following picture shows the plot of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6568527",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe = df_performances.copy()\n",
    "cleaned_dataframe[\"LABEL\"] = labels\n",
    "sns.pairplot(data = cleaned_dataframe, hue = \"LABEL\", palette = \"Accent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ad95c",
   "metadata": {},
   "source": [
    "**choice of parameters** <br>\n",
    "To choose the best parameters more combination of eps and min samples are analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf31874",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pdist(X=scaled_dataframe, metric='euclidean')  # pair-wise distance: how every record is far from all others\n",
    "dist = squareform(dist)                      # distance matrix given the vector dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ff84c",
   "metadata": {},
   "source": [
    "Calculate the distance of the K-th neighbour, in order to find an upper bound for the eps values to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafad21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin, kmax = 1, 50\n",
    "kth_distances = {}\n",
    "for k in range(kmin, kmax + 1): # initialize k lists\n",
    "    kth_distances[k] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83045abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dist:\n",
    "    # argsort returns the indexes that would sort d\n",
    "    index_kth_distance = np.argsort(d)[k]\n",
    "    for k in range(kmin, kmax + 1):\n",
    "        # append to kth_distances[k] the value in d that corresponds\n",
    "        # to the distance of the i-th point (record) from its k-th nn.\n",
    "        # it's like: kth_distances[k].append(sorted_d[k])), but we get \"sorted_d[k]\" by d[indexes_to_sort_d[k]]\n",
    "        kth_distances[k].append(d[index_kth_distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5952f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 20))\n",
    "for k in kth_distances.keys():\n",
    "    plt.plot(range(0, len(kth_distances[k])), sorted(kth_distances[k]))\n",
    "    \n",
    "plt.ylabel('dist from k-th neighbor (eps)', fontsize=25)\n",
    "plt.xlabel('sorted distances', fontsize=25)\n",
    "#plt.ylim(top=5)\n",
    "plt.ylim(bottom=-0.25)\n",
    "plt.tick_params(axis='both', which='major', labelsize=25)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be54684",
   "metadata": {},
   "source": [
    "#### Grid search for eps and min_samples\n",
    "\n",
    "A function to define metrics to evaluate the parameters is defined. The define metrics are 2: the number of clusters obtained with the parameters and the mean noise between noise points and the 6 points closer (6-NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbcfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(eps, min_samples, dataset, iter_):\n",
    "    # Fitting \n",
    "    dbscan_model_ = DBSCAN(eps = eps, min_samples = min_samples)\n",
    "    dbscan_model_.fit(dataset)\n",
    "    \n",
    "    # Mean Noise Point Distance metric\n",
    "    noise_indices = dbscan_model_.labels_ == -1\n",
    "    \n",
    "    if True in noise_indices:\n",
    "        neighboors = NearestNeighbors(n_neighbors = 6).fit(dataset)\n",
    "        distances, indices = neighboors.kneighbors(dataset)\n",
    "        noise_distances = distances[noise_indices, 1:]\n",
    "        noise_mean_distance = round(noise_distances.mean(), 3)\n",
    "    else:\n",
    "        noise_mean_distance = None\n",
    "    \n",
    "    # Number of found Clusters metric    \n",
    "    number_of_clusters = len(set(dbscan_model_.labels_[dbscan_model_.labels_ >= 0]))\n",
    "    \n",
    "    #print(\"%3d | Tested with eps = %3s and min_samples = %3s | %5s %4s\" % (i, eps, min_samples, str(noise_mean_distance), number_of_clusters))\n",
    "    \n",
    "    return(noise_mean_distance, number_of_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021609a9",
   "metadata": {},
   "source": [
    "Definition of values that must be analyzed for eps and the min number of samples. Looking at the plot above the used threashold for eps is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321050ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_to_test = [round(x, 3) for x in np.arange(0.5, 3, 0.2)] \n",
    "min_samples_to_test = np.arange(4, 30, 2)\n",
    "print(eps_to_test)\n",
    "print(min_samples_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4721a0",
   "metadata": {},
   "source": [
    "Creation of dataframes that will contain the result of analysis (one for the mean noise and one for the number of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe per la metrica sulla distanza media dei noise points dai K punti più vicini\n",
    "results_noise = pd.DataFrame( \n",
    "    data = np.zeros((len(eps_to_test),len(min_samples_to_test))), # Empty dataframe\n",
    "    columns = min_samples_to_test, \n",
    "    index = eps_to_test\n",
    ")\n",
    "\n",
    "# Dataframe per la metrica sul numero di cluster\n",
    "results_clusters = pd.DataFrame( \n",
    "    data = np.zeros((len(eps_to_test),len(min_samples_to_test))), # Empty dataframe\n",
    "    columns = min_samples_to_test, \n",
    "    index = eps_to_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e1475",
   "metadata": {},
   "source": [
    "Analysis of chosed values for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search\n",
    "i = 0\n",
    "\n",
    "for eps in eps_to_test:\n",
    "    for min_samples in min_samples_to_test:\n",
    "        i += 1\n",
    "        # Calcolo le metriche\n",
    "        noise_metric, cluster_metric  = get_metrics(eps, min_samples, scaled_dataframe, i)\n",
    "        # Inserisco i risultati nei relativi dataframe\n",
    "        results_noise.loc[eps, min_samples] = noise_metric\n",
    "        results_clusters.loc[eps, min_samples] = cluster_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6249c2",
   "metadata": {},
   "source": [
    "Plot of the results of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8) )\n",
    "\n",
    "sns.heatmap(results_noise, annot = True, ax = ax1, cbar = False).set_title(\"Mean Noise Points Distance\")\n",
    "sns.heatmap(results_clusters, annot = True, ax = ax2, cbar = False).set_title(\"Number of clusters\")\n",
    "\n",
    "ax1.set_xlabel(\"min_samples\")\n",
    "ax2.set_xlabel(\"min_samples\")\n",
    "ax1.set_ylabel(\"eps\")\n",
    "ax2.set_ylabel(\"eps\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70b01f",
   "metadata": {},
   "source": [
    "#### choice of parameters \n",
    "\n",
    "Analyzing the tables we choose eps=1.1 and min_samples=16 to obtain 3 clusters plus the one for unclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dbscan = DBSCAN(eps = 1.1, min_samples = 16)\n",
    "# Fitting\n",
    "best_dbscan.fit(scaled_dataframe)\n",
    "\n",
    "labels = best_dbscan.labels_\n",
    "np.unique(best_dbscan.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252d74a",
   "metadata": {},
   "source": [
    "Plot of the new clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels\n",
    "cleaned_dataframe = df_performances.copy()\n",
    "cleaned_dataframe[\"LABEL\"] = best_dbscan.labels_\n",
    "# Pairplot\n",
    "sns.pairplot(data = cleaned_dataframe, hue = \"LABEL\", palette = \"Accent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1cd77",
   "metadata": {},
   "source": [
    "**Normalization** <br />\n",
    "We normalize the values using MinMaxScaler in order to have the same scale on all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c121e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_array = scaler.fit_transform(df_performances)\n",
    "scaled_dataframe = pd.DataFrame( scaled_array, columns = df_performances.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca85ff",
   "metadata": {},
   "source": [
    "**example of clustering with fixed parameters eps and min points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.75, min_samples=5)\n",
    "dbscan.fit(scaled_dataframe)\n",
    "labels = dbscan.labels_\n",
    "np.unique(dbscan.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e2199",
   "metadata": {},
   "source": [
    "**choice of parameters** <br>\n",
    "To choose the best parameters more combination of eps and min samples are analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8178d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pdist(X=scaled_dataframe, metric='euclidean')  # pair-wise distance: how every record is far from all others\n",
    "dist = squareform(dist)                      # distance matrix given the vector dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e577bd",
   "metadata": {},
   "source": [
    "Calculate the distance of the K-th neighbour, in order to find an upper bound for the eps values to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin, kmax = 3, 50\n",
    "kth_distances = {}\n",
    "for k in range(kmin, kmax + 1): # initialize k lists\n",
    "    kth_distances[k] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dist:\n",
    "    # argsort returns the indexes that would sort d\n",
    "    index_kth_distance = np.argsort(d)[k]\n",
    "    for k in range(kmin, kmax + 1):\n",
    "        # append to kth_distances[k] the value in d that corresponds\n",
    "        # to the distance of the i-th point (record) from its k-th nn.\n",
    "        # it's like: kth_distances[k].append(sorted_d[k])), but we get \"sorted_d[k]\" by d[indexes_to_sort_d[k]]\n",
    "        kth_distances[k].append(d[index_kth_distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57775ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 20))\n",
    "for k in kth_distances.keys():\n",
    "    plt.plot(range(0, len(kth_distances[k])), sorted(kth_distances[k]))\n",
    "    \n",
    "plt.ylabel('dist from k-th neighbor (eps)', fontsize=25)\n",
    "plt.xlabel('sorted distances', fontsize=25)\n",
    "#plt.ylim(top=5)\n",
    "plt.ylim(bottom=-0.25)\n",
    "plt.tick_params(axis='both', which='major', labelsize=25)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c5884",
   "metadata": {},
   "source": [
    "Definition of values that must be analyzed for eps and the min number of samples. Looking at the plot above the used threashold for eps is 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_to_test = [round(x, 3) for x in np.arange(0.1, 0.75, 0.1)]\n",
    "min_samples_to_test = np.arange(2, 16, 1)\n",
    "print(eps_to_test)\n",
    "print(min_samples_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c46e2",
   "metadata": {},
   "source": [
    "Creation of dataframes that will contain the result of analysis (one for the mean noise and one for the number of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe per la metrica sulla distanza media dei noise points dai K punti più vicini\n",
    "results_noise = pd.DataFrame( \n",
    "    data = np.zeros((len(eps_to_test),len(min_samples_to_test))), # Empty dataframe\n",
    "    columns = min_samples_to_test, \n",
    "    index = eps_to_test\n",
    ")\n",
    "\n",
    "# Dataframe per la metrica sul numero di cluster\n",
    "results_clusters = pd.DataFrame( \n",
    "    data = np.zeros((len(eps_to_test),len(min_samples_to_test))), # Empty dataframe\n",
    "    columns = min_samples_to_test, \n",
    "    index = eps_to_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bd5bd",
   "metadata": {},
   "source": [
    "Analysis of chosed values for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b86eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for eps in eps_to_test:\n",
    "    for min_samples in min_samples_to_test:\n",
    "        i += 1\n",
    "        # Calcolo le metriche\n",
    "        noise_metric, cluster_metric  = get_metrics(eps, min_samples, scaled_dataframe, i)\n",
    "        # Inserisco i risultati nei relativi dataframe\n",
    "        results_noise.loc[eps, min_samples] = noise_metric\n",
    "        results_clusters.loc[eps, min_samples] = cluster_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb024a19",
   "metadata": {},
   "source": [
    "Plot of the results of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa47e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,4) )\n",
    "\n",
    "sns.heatmap(results_noise, annot = True, ax = ax1, cbar = False).set_title(\"Mean Noise Points Distance\")\n",
    "sns.heatmap(results_clusters, annot = True, ax = ax2, cbar = False).set_title(\"Number of clusters\")\n",
    "\n",
    "ax1.set_xlabel(\"min_samples\")\n",
    "ax2.set_xlabel(\"min_samples\")\n",
    "ax1.set_ylabel(\"eps\")\n",
    "ax2.set_ylabel(\"eps\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe0a5c",
   "metadata": {},
   "source": [
    "#### choice of parameters \n",
    "\n",
    "Analyzing the tables we choose eps=0.1 and min_samples=9 to obtain 4 clusters plus the one for unclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dbscan = DBSCAN(eps = 0.1, min_samples = 9)\n",
    "# Fitting\n",
    "best_dbscan.fit(scaled_dataframe)\n",
    "\n",
    "labels = best_dbscan.labels_\n",
    "np.unique(best_dbscan.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc5c1d",
   "metadata": {},
   "source": [
    "Plot of the new clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02063ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels\n",
    "cleaned_dataframe = df_performances.copy()\n",
    "cleaned_dataframe[\"LABEL\"] = best_dbscan.labels_\n",
    "# Pairplot\n",
    "sns.pairplot(data = cleaned_dataframe, hue = \"LABEL\", palette = \"Accent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd6881",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Analizyng wih both scalers we obtain a big cluster for unclassified points. With **StandardScaler**, this cluster has more than 400 points. About other clusters, one of them has many points (about 800) but the others have less then 20 points, so we can notice that clusters are very unbalanced. With **MinMaxScaler** the cluster of unclassified points is  even bigger with about 980 points. But some of the other clusters are relatively big (about 100 and 150 points), and some are small. So with this scaler there is not only a big cluster very big respect to the others but there are more bigger clusters and the difference between dimension of all clusters (except the unclassified one) is smaller.\n",
    "To choose different parameters does not help to obtain better clusters. The results are not as clearly describable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f183c5c",
   "metadata": {},
   "source": [
    "# Hierachical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a7ed9",
   "metadata": {},
   "source": [
    "In this section we will see hierarchical clustering performed the divisive technique, in particular, using the four methods to compute the distances between clusters. To compute the distances has been used euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49117fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cluster_elements(data, threshold, criterion='distance'):\n",
    "    count = {}\n",
    "    clusters = fcluster(data, threshold, criterion)\n",
    "    for c in clusters:\n",
    "        count[c] = count[c]+1 if c in count else 1\n",
    "    return count, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['single', 'complete', 'average', 'ward']\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62276e",
   "metadata": {},
   "source": [
    "Using scipy we can compute the linkage matrix and use it for the dendrogram plot. This is done for all the criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e962354",
   "metadata": {},
   "source": [
    "- ‘**single**’ uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "- ‘**complete**’ or ‘**maximum**’ linkage uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "- ‘**average**’ uses the average of the distances of each observation of the two sets.\n",
    "\n",
    "- ‘**ward**’ minimizes the variance of the clusters being merged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79835819",
   "metadata": {},
   "source": [
    "**Normalization** <br />\n",
    "We normalize the values using MinMaxScaler in order to have the same scale on all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_array = scaler.fit_transform(df_performances)\n",
    "scaled_dataframe = pd.DataFrame( scaled_array, columns = df_performances.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    link = linkage(scaled_dataframe, method=method, metric = 'euclidean')\n",
    "    models[method] = link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a624f",
   "metadata": {},
   "source": [
    "Now plot the four dendrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797df81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(ncols=4, figsize=(32,7))\n",
    "i = 0\n",
    "for model in models.keys():\n",
    "    axs[i].set_title('Hierarchical Clustering by ' + model + ' Algorithm (' + methods[i] + '-linkage)')\n",
    "    axs[i].set_xlabel('Players or (Cluster Size)')\n",
    "    axs[i].set_ylabel('Distance')\n",
    "    dend = dendrogram(models[model],ax=axs[i],truncate_mode='lastp', p=25, leaf_rotation=60, leaf_font_size = 8, show_contracted=True)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea682b0",
   "metadata": {},
   "source": [
    "Using this function we can identify the longest uninterrupted vertical segment of the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_cut(model, scaled_dataframe):\n",
    "    store_n_clusters = []\n",
    "    store_distances = []\n",
    "    step = 0.05\n",
    "    n_clusters = 0\n",
    "    i = 0\n",
    "\n",
    "    while n_clusters != 1:\n",
    "        distance_threshold = i*step\n",
    "        cluster = AgglomerativeClustering(distance_threshold=distance_threshold, n_clusters=None, affinity='euclidean', linkage=model)\n",
    "        cluster.fit_predict(scaled_dataframe)\n",
    "        i = i + 1\n",
    "\n",
    "        n_clusters = cluster.n_clusters_\n",
    "        store_distances.append(distance_threshold)\n",
    "        store_n_clusters.append(n_clusters)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['distance_thresholds'] = store_distances\n",
    "    df['n_clusters'] = store_n_clusters\n",
    "    df_n_clusters_groups = df.groupby('n_clusters').size()\n",
    "    n_for_opt_cut = df_n_clusters_groups.agg(['idxmax'])[0]\n",
    "    distance = df[df.n_clusters == n_for_opt_cut].distance_thresholds.min()\n",
    "    \n",
    "    return n_for_opt_cut, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373520ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_thresholds = {}\n",
    "for model in models.keys():\n",
    "    n_for_opt_cut, distance = compute_best_cut(model, scaled_dataframe)\n",
    "    cut_thresholds[model] = distance\n",
    "print(cut_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(ncols=4, figsize=(32,7))\n",
    "i = 0\n",
    "for model in models.keys():\n",
    "    axs[i].set_title('Hierarchical Clustering by ' + model + ' Algorithm (' + methods[i] + '-linkage)')\n",
    "    axs[i].set_xlabel('Players or (Cluster Size)')\n",
    "    axs[i].set_ylabel('Distance')\n",
    "    axs[i].axhline(y=cut_thresholds[model], color=\"black\")\n",
    "    dend = dendrogram(models[model],ax=axs[i],truncate_mode='lastp', p=25, leaf_rotation=60, leaf_font_size = 8, show_contracted=True)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a849ffc",
   "metadata": {},
   "source": [
    "**Clusters evalution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e0ca2",
   "metadata": {},
   "source": [
    "Let's compute some metrics to evaluate the clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95f5d1",
   "metadata": {},
   "source": [
    " **Silhouette coefficient**: defines how good the clusters are sperated. The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n",
    " \n",
    " \n",
    " **Davies-Bouldin index**: means the average ‘similarity’ between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Values closer to zero indicate a better partition.\n",
    " \n",
    " \n",
    " **Calinski and Harabasz score**: defined as ratio between the within-cluster dispersion and the between-cluster dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labels = {}\n",
    "metrics = ['silhouette', 'davies_bouldin', 'calinski_harabasz']\n",
    "evaluation_metrics = pd.DataFrame(index=methods, columns=metrics)\n",
    "for model in models.keys():\n",
    "    print(model)\n",
    "    count, clusters = count_cluster_elements(models[model], cut_thresholds[model])\n",
    "    print(count)\n",
    "    \n",
    "    cluster = AgglomerativeClustering(n_clusters=len(count.keys()), affinity='euclidean', linkage=model)\n",
    "    cluster.fit_predict(scaled_dataframe)\n",
    "    labels = cluster.labels_\n",
    "    model_labels[model] = labels\n",
    "    silhouette = silhouette_score(scaled_dataframe, labels, metric='euclidean')\n",
    "    davies_bouldin = davies_bouldin_score(scaled_dataframe, labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(scaled_dataframe, labels)\n",
    "    evaluation_metrics.loc[model][metrics[0]] = silhouette\n",
    "    evaluation_metrics.loc[model][metrics[1]] = davies_bouldin\n",
    "    evaluation_metrics.loc[model][metrics[2]] = calinski_harabasz\n",
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac4963",
   "metadata": {},
   "source": [
    "In the first part of the outputis showed the number of items for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd804e9",
   "metadata": {},
   "source": [
    "All three matrics show that the clustering has high difficulty to define well separated groups. Using the *single* metric the results improve, this is due to high similarity between the records, but are far away from a good clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f8ec6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "f, axs = plt.subplots(ncols=4, figsize=(32,7), sharey=True)\n",
    "for model in models.keys():\n",
    "    axs[i].set_title('Hierarchical Clustering by ' + model + ' Algorithm (' + methods[i] + '-linkage)')\n",
    "    axs[i].scatter(df_performances['nmatch'].values, df_performances['best_rank_points'].values, c=model_labels[model] , s=25, cmap='viridis')\n",
    "    axs[i].set_xlabel('nmatch')\n",
    "    axs[i].set_ylabel('best_rank_points')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0843c4",
   "metadata": {},
   "source": [
    "# XMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_initial_centers = 1   #number of clusters at the start. xmeans starts with only one cluster\n",
    "max_n_clusters = 20\n",
    "\n",
    "initial_centers = kmeans_plusplus_initializer(df_performances, amount_initial_centers).initialize()\n",
    "xmeans_instance = xmeans(df_performances, initial_centers, kmax=max_n_clusters)\n",
    "xmeans_instance.process(); #split with bayesian Information Criterion\n",
    "\n",
    "clusters = xmeans_instance.get_clusters();\n",
    "centers = xmeans_instance.get_centers()\n",
    "\n",
    "print([len(c) for c in clusters])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d14796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display allocated clusters\n",
    "visualizer = cluster_visualizer_multidim();\n",
    "visualizer.append_clusters(clusters, df_performances.values.tolist())\n",
    "visualizer.append_cluster(centers, None, marker = '*', markersize=5)\n",
    "#visualizer.show()\n",
    "visualizer.show(pair_filter=[[0, 1], [0, 2], [0, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bd541",
   "metadata": {},
   "source": [
    "**plot with sns library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros(df_performances.shape[0],  dtype=int) #num of rows\n",
    "for i in range(len(clusters)):#number of cluster\n",
    "    for j in clusters[i]: #index of row of dataset in cluster i\n",
    "        labels[j] = int(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_n = sns.color_palette(\"hls\", n_colors=len(clusters))\n",
    "palette_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbe76c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extracting labels\n",
    "cleaned_dataframe = df_performances.copy()\n",
    "cleaned_dataframe[\"LABEL\"] = labels\n",
    "# Pairplot\n",
    "sns.pairplot(data = cleaned_dataframe, hue = \"LABEL\", palette = palette_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee02e53",
   "metadata": {},
   "source": [
    "# G-Means clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb5d22",
   "metadata": {},
   "source": [
    "<a href=\"https://pyclustering.github.io/docs/0.10.1/html/dc/d86/namespacepyclustering_1_1cluster_1_1gmeans.html\">G-MEANS by PYCLUSTERING</a>\n",
    "<br />\n",
    "<i>Greg Hamerly and Charles Elkan. Learning the k in k-means. In Proceedings of the 16th International Conference on Neural Information Processing Systems, NIPS'03, pages 281–288, Cambridge, MA, USA, 2003. MIT Press.</i>\n",
    "<br /><br />\n",
    "The G-means algorithm starts with a small number of centers, and grows the number of centers. Each iteration of the G-Means algorithm splits into two those centers whose data appear not to come from a Gaussian distribution. G means repeatedly makes decisions based on a statistical test for the data assigned to each center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fedf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df_performances.values.tolist()\n",
    "\n",
    " # Create instance of G-Means algorithm. By default the algorithm starts search from a single cluster.\n",
    "max_n_clusters = 20\n",
    "gmeans_instance = gmeans(data, k_max=max_n_clusters).process()\n",
    " \n",
    "# Extract clustering results: clusters and their centers\n",
    "clusters = gmeans_instance.get_clusters()\n",
    "centers = gmeans_instance.get_centers()\n",
    " \n",
    "# Print total sum of metric errors\n",
    "print(\"Total WCE:\", gmeans_instance.get_total_wce())\n",
    "\n",
    "print([len(c) for c in clusters]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02011513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize a sample of obtained clusters\n",
    "visualizer = cluster_visualizer_multidim()\n",
    "visualizer.append_clusters(clusters, data = df_performances.values.tolist())\n",
    "visualizer.show(pair_filter=[[0, 1], [0, 2], [0, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print palette colors for clusters\n",
    "palette_n = sns.color_palette(\"hls\", n_colors=len(clusters))\n",
    "palette_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262aa15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "labels = np.zeros(df_performances.shape[0],  dtype=int) #num of rows\n",
    "for i in range(len(clusters)):#number of cluster\n",
    "    for j in clusters[i]: #index of row of dataset in cluster i\n",
    "        labels[j] = int(i)\n",
    "\n",
    "# Print \n",
    "cleaned_dataframe = df_performances.copy()\n",
    "cleaned_dataframe[\"LABEL\"] = labels\n",
    "# Pairplot\n",
    "sns.pairplot(data = cleaned_dataframe, hue = \"LABEL\", palette = palette_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ec3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
